model:
  default_preset: claude-haiku
  # default_preset: gpt-4o-mini
  # default_preset: o1-mini
  # default_preset: o1-preview
  # default_preset: gemini-1.5-pro-002
  # default_preset: llama3-405b
  # default_preset: claude-sonnet
  # default_preset: gpt-4-chatbot-responses
  # default_preset: claude-chatbot-responses
  # default_preset: mistral-large
chat:
  log:
    enabled: true
    # Provide as CLI arg, /dev/null is used here to prevent.
    # the file from being written if it's turned on in a
    # default Config().
    filepath: /dev/null
debug:
  log:
    enabled: true
plugins:
  enabled:
    # Built-in plugins
    # - echo
    # - examples
    # - provider_chat_openai_compat
    - provider_fake_llm
    # Installed plugins.
    # - awesome
    # - data_query
    # - database
    - gist
    # - pastebin
    - shell
    # - test
    # Installed provider plugins.
    # - provider_ai21
    # - provider_azure_openai_chat
    - provider_chat_anthropic
    # - provider_chat_anyscale
    # - provider_chat_cohere
    # - provider_chat_fireworks
    - provider_chat_google_genai
    # - provider_chat_groq
    # - provider_chat_mistralai
    # - provider_chat_ollama
    # - provider_chat_together
    # - provider_chat_vertexai
    # - provider_huggingface_hub
    # - provider_openai
    - provider_openrouter
    # - provider_vertexai
  examples:
    confirm_overwrite: false
  pastebin:
    include_raw_link: true
    exclude_system_messages: true
  gist:
    include_raw_link: true
    exclude_system_messages: true
  provider_openrouter:
    models:
      liquid/lfm-40b:
        max_tokens: 32768
  #     deepseek/deepseek-coder:
  #       max_tokens: 32768
  # provider_chat_fireworks:
  #   models:
  #     "accounts/fireworks/models/mixtral-8x22b-instruct":
  #       max_tokens: 65536
  #     "accounts/fireworks/models/llama-v3-70b-instruct":
  #       max_tokens: 8192
  #     "accounts/fireworks/models/firefunction-v1":
  #       max_tokens: 32768
  # provider_chat_together:
  #   models:
  #     "mistralai/Mixtral-8x7B-Instruct-v0.1":
  #       max_tokens: 32768
  #     "meta-llama/Llama-3-70b-chat-hf":
  #       max_tokens: 8192
  # provider_chat_groq:
  #   show_inactive_models: true
